{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "import seaborn as sns\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import cycle\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making all figures\n",
    " - Test error (PLSR & RF)\n",
    " - Clustering (DE genes) -> GO processes (done)\n",
    " - PCA (DE genes)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## ALREADY PREPROCESSED \n",
    "#### Min gene count = 0 for 15% of samples\n",
    "\n",
    "train_fpkm = pd.read_csv(\"../data/BAL/train_fpkm_minfilter0_log2.csv\",sep='\\t',index_col=0)\n",
    "train_meta = pd.read_csv('../data/BAL/Reseq_ALL/meta_reseq.csv', sep='\\t', index_col='sample_id')\n",
    "\n",
    "test_fpkm = pd.read_csv(\"../data/BAL/test_fpkm_minfilter0_log2.csv\", sep='\\t', index_col=0)\n",
    "test_meta = pd.read_csv('../data/BAL/meta_r0.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 26081) (32, 2)\n",
      "(95, 11957) (95, 32)\n"
     ]
    }
   ],
   "source": [
    "print test_fpkm.shape, test_meta.shape\n",
    "print train_fpkm.shape, train_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_meta['sum_fpkm'] = train_fpkm.apply(lambda x: np.sum(x), axis=1)\n",
    "test_meta['sum_fpkm'] = test_fpkm.apply(lambda x: np.sum(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_train_fpkm = np.mean(train_meta['sum_fpkm'])\n",
    "mean_test_fpkm = np.mean(test_meta['sum_fpkm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = mean_train_fpkm / mean_test_fpkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fpkm_norm = train_fpkm.apply(lambda x: x / ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fpkm = train_fpkm_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geteven_xy(df, dfm, celltype, sampling='up', norm=False):\n",
    "    dfm = dfm[dfm['CellType']==celltype]\n",
    "    y = dfm['Pneum'].values\n",
    "    df = df.ix[dfm.index]\n",
    "    #    if z: df = df.apply(lambda x: zscore(x))\n",
    "    genes = df.columns\n",
    "    pos = [i for i in range(len(y)) if y[i]==True]\n",
    "    neg = [i for i in range(len(y)) if y[i]==False]\n",
    "    \n",
    "    npos = len(pos)\n",
    "    nneg = len(neg)\n",
    "\n",
    "    #print \"Num positive: {}\".format(len(pos))\n",
    "    #print \"Num negative: {}\".format(len(neg))\n",
    "    if sampling=='down':\n",
    "        if nneg >= npos:\n",
    "            neg = random.sample(neg, npos)\n",
    "        else:\n",
    "            pos = random.sample(pos, nneg)\n",
    "    if sampling=='up':\n",
    "        if nneg >= npos:\n",
    "            dup_pos = list(np.random.choice(pos, nneg-npos))\n",
    "            pos = pos + dup_pos ## pos+pos in case not enough in pos\n",
    "    #            print \"duplicated pos:\", dup_pos\n",
    "        else:\n",
    "            neg = neg + np.random.choice(neg, npos-nneg)\n",
    "\n",
    "    df = df.ix[(pos+neg),:]\n",
    "    if norm=='zero_one': \n",
    "        for col in df.columns:\n",
    "            mx = np.max(df[col])\n",
    "            mn = np.min(df[col])\n",
    "            df[col] = df[col].map(lambda x: (x - mn)/(mx - mn))\n",
    "            \n",
    "    if norm=='zscore': \n",
    "        df = df.apply(lambda x: zscore(x))\n",
    "\n",
    "    dfm = dfm.ix[(pos+neg),:]\n",
    "    X = np.array(df)\n",
    "    y = dfm['Pneum'].values\n",
    "    print \"Num pos after sampling: {}\".format(len(pos))\n",
    "    print \"Num neg after sampling: {}\".format(len(neg))\n",
    "    #y = y[pos + neg] \n",
    "    #X = X[pos + neg] \n",
    "    return X,y, genes, df, dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(celltype, n_top_genes, method='PLSR', \n",
    "                       norm=False, random_genes=False, jumble_test=False, jumble_train=False,\n",
    "                       n_jumbles=20, npcs=5, n_estimators=100, sampling=False, cross_validation='LOO', zero_train=False):\n",
    "      \n",
    "    '''\n",
    "    Retrieves training data, gets top features using cross-validation.\n",
    "    Retreives test data, filters to top features, predicts.\n",
    "    '''\n",
    "    \n",
    "    ### TRAINING\n",
    "    #################\n",
    "        \n",
    "    print \"Model = \", celltype\n",
    "    print \"Method = \", method\n",
    "    #print \"N_Top_Genes = \", n_top_genes\n",
    "    print \"Normalization = \", norm\n",
    "    \n",
    "    # Get training data\n",
    "    X, y, genes, df, dfm = geteven_xy(train_fpkm, train_meta, celltype=celltype, norm=norm, sampling=sampling)\n",
    "    \n",
    "    if jumble_train:\n",
    "        print \"---- Entire training set is jumbled ----\\n\"\n",
    "        df = df.apply(np.random.permutation)\n",
    "        X = np.array(df)\n",
    "    if zero_train:\n",
    "        df = df.apply(lambda x: x*0)\n",
    "        X = np.array(df)\n",
    "\n",
    "    # Get top features using cross-validation\n",
    "    ##############\n",
    "    neg_err, pos_err, Q2, neg_accuracy_train, pos_accuracy_train, feature_inds = calc_metrics(X, y, \n",
    "                            method=method, cross_validation=cross_validation, \n",
    "                            n_pcs=npcs, n_estimators=n_estimators, n_top_genes=n_top_genes)\n",
    "\n",
    "    ## TRAIN METRICS\n",
    "    print \"Q Squared: {0:.2f} \\n\".format(Q2)\n",
    "    num_neg_correct = sum([(e < 0.5) for e in neg_err])\n",
    "    num_pos_correct = sum([(e < 0.5) for e in pos_err])\n",
    "    \n",
    "    print \"\\n===== Training ======\"\n",
    "    print \"Negative: {0:.3f} ({1}/{2}) \".format(neg_accuracy_train, num_neg_correct, len(neg_err))\n",
    "    print \"Positive: {0:.3f} ({1}/{2}) \".format(pos_accuracy_train, num_pos_correct, len(pos_err))\n",
    "    \n",
    "    \n",
    "    ## Get most common genes from each LOO model\n",
    "    ##############\n",
    "    all_top_feature_inds = [g for m in feature_inds for g in m]\n",
    "    c = Counter(all_top_feature_inds)\n",
    "    top_inds = [i[0] for i in c.most_common(n_top_genes)]\n",
    "    top_genes_train = [genes[i] for i in top_inds]\n",
    "    \n",
    "    \n",
    "    ## SWITCH OUT TOP GENES WITH RANDOM GENES\n",
    "    ##############\n",
    "    if random_genes:\n",
    "        top_genes_train = np.random.choice(genes, n_top_genes)\n",
    "        print \"\\n **Genes for testing are RANDOM** \\n\"\n",
    "\n",
    "    \n",
    "    ### BUILDING PREDICTIVE MODEL with top genes\n",
    "    ##############\n",
    "    train_spreads = col_spreads(df[top_genes_train])\n",
    "    \n",
    "    \n",
    "    X_train = np.array(df[top_genes_train])\n",
    "    y_train = np.array(dfm['Pneum'])\n",
    "    \n",
    "    \n",
    "    ### GET TEST SET, FILTER TO HAVE ONLY TOP FEATURES\n",
    "    ############\n",
    "    X_test, y_test, genes_test, df_test, dfm_test = geteven_xy(test_fpkm, test_meta, celltype=celltype, norm=norm, sampling=False)\n",
    "    \n",
    "    test_spreads = col_spreads(df_test[top_genes_train])\n",
    "    X_test = np.array(df_test[top_genes_train])\n",
    "    \n",
    "    ### TESTING WITH ACTUAL (non-jumbled) GENES\n",
    "    ###############\n",
    "    print \"\\n===== Testing =====\"\n",
    "    \n",
    "    if method=='PLSR': \n",
    "        model = PLSRegression(5, scale=True)\n",
    "    elif method=='RandomForests': \n",
    "        model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    else: \n",
    "        print \"Method {} not recognized\".format(method)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_accs = prediction_stats(y_test, y_pred, print_stats=True)\n",
    "\n",
    "    return train_spreads, test_spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### spread: Max - min\n",
    "def col_spreads(df):\n",
    "    spreads = []\n",
    "    for c in df.columns:\n",
    "        mx, mn = np.max(df[c]), np.min(df[c])\n",
    "        spreads.append(mx-mn)\n",
    "    return spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_stats(y_test, y_pred, print_stats=False):\n",
    "    \n",
    "    neg_err = [float(abs(y_pred[i]-y_test[i])) for i in range(len(y_pred)) if y_test[i]==0]\n",
    "    num_neg_correct = sum([(e < 0.5) for e in neg_err])\n",
    "    neg_accuracy_test = float(num_neg_correct) / len(neg_err)\n",
    "\n",
    "\n",
    "    pos_err = [float(abs(y_pred[i]-y_test[i])) for i in range(len(y_pred)) if y_test[i]==1]\n",
    "    num_pos_correct = sum([(e < 0.5) for e in pos_err])\n",
    "    pos_accuracy_test = float(num_pos_correct) / len(pos_err)\n",
    "    \n",
    "    if print_stats:\n",
    "        print \"Negative: {0:.3f} ({1}/{2}) \".format(neg_accuracy_test, num_neg_correct, len(neg_err))        \n",
    "        print \"Positive: {0:.3f} ({1}/{2}) \".format(pos_accuracy_test, num_pos_correct, len(pos_err))\n",
    "\n",
    "    return neg_accuracy_test, pos_accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layout for analysis:\n",
    " 1. Get test accuracies, random=False, sampling=False, jumble_train=False\n",
    " 2. Is there meaning in the genes? -> change [1] to have jumble_train=True\n",
    " 3. Is the model based on the top 100 genes better than based on random genes?\n",
    " 4. Is the model based on the top 100 genes better than the jumbled 100 genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_metrics(X, y, n_pcs, n_estimators, n_top_genes, method='PLSR', cross_validation='LOO', k=5, n_splits=10):\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    pred = []\n",
    "    feat_inds_all = []\n",
    "    #print X.shape, y.shape\n",
    "    \n",
    "    ### Leave-One-Out Cross-validation\n",
    "    \n",
    "    if cross_validation=='LOO':\n",
    "        for sample in range(n_samples):\n",
    "            samples = range(n_samples)\n",
    "            samples.remove(sample)\n",
    "            X_train = X[(samples)]\n",
    "            y_train = y[(samples)]\n",
    "            #print X_t.shape    \n",
    "\n",
    "            if method=='PLSR':### Get VIPs for model built without the LOO sample\n",
    "                plsv = PLSRegression(n_pcs, scale=False)\n",
    "                plsv.fit(X_train, y_train)\n",
    "                vips = vipp(X_train, y_train, plsv.x_scores_, plsv.x_weights_)\n",
    "                vips = [float(v) for v in vips]\n",
    "                vip_inds = np.argsort(vips)[::-1][:n_top_genes]\n",
    "                feat_inds_all.append(vip_inds)\n",
    "\n",
    "                ### Xn takes the original X to get the loo sample\n",
    "                X_top_features = X.T[vip_inds].T\n",
    "                loo_sample = np.array(X_top_features[sample]).reshape((1,-1))\n",
    "\n",
    "                ### Filter the samples \n",
    "                X_train_top_features = X_train.T[vip_inds].T\n",
    "                pls = PLSRegression(n_pcs, scale=False)\n",
    "                pls.fit(X_train_top_features, y_train)\n",
    "                pred.append(float(pls.predict(loo_sample)))\n",
    "\n",
    "\n",
    "            elif method=='RandomForests':        ### Get VIPs for model built without the LOO sample\n",
    "                rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "                rfc.fit(X_train, y_train)\n",
    "                feat_imp = rfc.feature_importances_\n",
    "                feat_inds = np.argsort(feat_imp)[::-1][:n_top_genes]\n",
    "                feat_inds_all.append(feat_inds)\n",
    "                \n",
    "                ### Xn takes the original X to get the loo sample\n",
    "                X_top_features = X.T[feat_inds].T\n",
    "                loo_sample = np.array(X_top_features[sample]).reshape((1,-1))\n",
    "\n",
    "                ### Filter the samples \n",
    "                X_train_top_features = X_train.T[feat_inds].T\n",
    "                rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "                rfc.fit(X_train_top_features, y_train)\n",
    "\n",
    "                #print \"prediction: \", float(rfc.predict(loo_sample))\n",
    "                pred.append(float(rfc.predict(loo_sample)))\n",
    "\n",
    "            else:\n",
    "                print \"Method {} not found\".format(method)\n",
    "\n",
    "    elif cross_validation=='Kfold':\n",
    "        y_k = []\n",
    "        for ki in range(n_splits):\n",
    "            sample_ind = range(n_samples)\n",
    "            withheld_samples = random.sample(sample_ind, k)\n",
    "            lo_samples = X[withheld_samples]\n",
    "            for s in withheld_samples:\n",
    "                sample_ind.remove(s)\n",
    "            X_train = X[(sample_ind)]\n",
    "            y_train = y[(sample_ind)]\n",
    "            \n",
    "            if method=='PLSR':\n",
    "                model = PLSRegression(n_pcs, scale=False)\n",
    "                model.fit(X_train, y_train)\n",
    "                vips = vipp(X_train, y_train, model.x_scores_, model.x_weights_)\n",
    "                vips = [float(v) for v in vips]\n",
    "                vip_inds = np.argsort(vips)[::-1][:n_top_genes]\n",
    "                feat_inds_all.append(vip_inds)\n",
    "\n",
    "                ### Filter the samples \n",
    "                X_train_top_features = X_train.T[vip_inds].T\n",
    "                model = PLSRegression(n_pcs, scale=False)\n",
    "                model.fit(X_train_top_features, y_train)\n",
    "                #pred.append(float(.predict(loo_sample)))\n",
    "                \n",
    "                ### Xn takes the original X to get the loo sample\n",
    "                X_top_features = X.T[vip_inds].T\n",
    "                \n",
    "            elif method=='RandomForest':\n",
    "                model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "                feat_imp = rfc.feature_importances_\n",
    "                feat_inds = np.argsort(feat_imp)[::-1][:n_top_genes]\n",
    "                feat_inds_all.append(feat_inds)\n",
    "                \n",
    "                ### Filter the samples \n",
    "                X_train_top_features = X_train.T[vip_inds].T\n",
    "                model = PLSRegression(n_pcs, scale=False)\n",
    "                model.fit(X_train_top_features, y_train)\n",
    "                \n",
    "                X_top_features = X.T[vip_inds].T\n",
    "                \n",
    "            elif method=='SVM':\n",
    "                model = SVC()\n",
    "            else:\n",
    "                print \"Method not found\"\n",
    "            \n",
    "            for s in withheld_samples:\n",
    "                pred.append(float(model.predict(X_top_features[s].reshape((1, -1)))))\n",
    "                y_k.append(y[s])\n",
    "        y = y_k\n",
    "\n",
    "    # Calculate metrics\n",
    "    ### Q squared\n",
    "    num = sum([float((pred[i] - y[i]))**2 for i in range(len(pred))])\n",
    "    den = sum([(y[i] - np.mean(y))**2 for i in range(len(pred))])\n",
    "    Q2 = float(1 - num/den)\n",
    "\n",
    "    ### Prediction error\n",
    "    errs = [abs(float((pred[i] - y[i]))) for i in range(len(pred))]\n",
    " \n",
    "    ### Percent correct for neg and pos Pneumonia\n",
    "    neg_err = [errs[i] for i in range(len(errs)) if y[i]==0]  \n",
    "    neg_corr = [e < 0.5 for e in neg_err]   \n",
    "    neg_corr = float(sum(neg_corr)) / len(neg_err)\n",
    "\n",
    "    \n",
    "    pos_err = [errs[i] for i in range(len(errs)) if y[i]==1]\n",
    "    pos_corr = [e < 0.5 for e in pos_err]\n",
    "    pos_corr = float(sum(pos_corr)) / len(pos_err)\n",
    "    \n",
    "\n",
    "    #print \"num: {0: .3f}, den: {1: .3f}, \n",
    "    #print \"Q^2: {0: .3f}\".format(Q2)\n",
    "    return neg_err, pos_err, Q2, neg_corr, pos_corr, feat_inds_all\n",
    "    #return [float(abs(pred[p] - y[p])) for p in range(len(pred))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Just the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  AM\n",
      "Method =  RandomForests\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 19\n",
      "Num neg after sampling: 30\n",
      "Q Squared: -1.15 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.567 (17/30) \n",
      "Positive: 0.368 (7/19) \n",
      "Num pos after sampling: 6\n",
      "Num neg after sampling: 14\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.929 (13/14) \n",
      "Positive: 0.667 (4/6) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='AM', n_top_genes=100, jumble_train=False, zero_train=False, method='RandomForests', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  AM\n",
      "Method =  PLSR\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 19\n",
      "Num neg after sampling: 30\n",
      "Q Squared: -0.33 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.700 (21/30) \n",
      "Positive: 0.105 (2/19) \n",
      "Num pos after sampling: 6\n",
      "Num neg after sampling: 14\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.857 (12/14) \n",
      "Positive: 0.667 (4/6) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='AM', n_top_genes=100, jumble_train=False, zero_train=False, method='PLSR', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  CD163\n",
      "Method =  RandomForests\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 15\n",
      "Num neg after sampling: 31\n",
      "Q Squared: -0.48 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.903 (28/31) \n",
      "Positive: 0.200 (3/15) \n",
      "Num pos after sampling: 7\n",
      "Num neg after sampling: 5\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 1.000 (5/5) \n",
      "Positive: 0.429 (3/7) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='CD163', n_top_genes=100, jumble_train=False, zero_train=False, method='RandomForests', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  CD163\n",
      "Method =  PLSR\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 15\n",
      "Num neg after sampling: 31\n",
      "Q Squared: -0.13 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.968 (30/31) \n",
      "Positive: 0.067 (1/15) \n",
      "Num pos after sampling: 7\n",
      "Num neg after sampling: 5\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 1.000 (5/5) \n",
      "Positive: 0.286 (2/7) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='CD163', n_top_genes=100, jumble_train=False, zero_train=False, method='PLSR', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  AM\n",
      "Method =  RandomForests\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 19\n",
      "Num neg after sampling: 30\n",
      "---- Entire training set is jumbled ----\n",
      "\n",
      "Q Squared: -0.63 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.933 (28/30) \n",
      "Positive: 0.105 (2/19) \n",
      "Num pos after sampling: 6\n",
      "Num neg after sampling: 14\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.786 (11/14) \n",
      "Positive: 0.333 (2/6) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='AM', n_top_genes=100, \n",
    "                   jumble_train=True, zero_train=False, method='RandomForests', \n",
    "                    norm='zscore', random_genes=False, jumble_test=False, \n",
    "                   sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  AM\n",
      "Method =  RandomForests\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 19\n",
      "Num neg after sampling: 30\n",
      "---- Entire training set is jumbled ----\n",
      "\n",
      "Q Squared: -0.63 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.733 (22/30) \n",
      "Positive: 0.421 (8/19) \n",
      "\n",
      " **Genes for testing are RANDOM** \n",
      "\n",
      "Num pos after sampling: 6\n",
      "Num neg after sampling: 14\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.857 (12/14) \n",
      "Positive: 0.333 (2/6) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='AM', n_top_genes=10, \n",
    "                   jumble_train=True, zero_train=False, method='RandomForests', \n",
    "                    norm='zscore', random_genes=True, jumble_test=False, \n",
    "                   sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  AM\n",
      "Method =  RandomForests\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 19\n",
      "Num neg after sampling: 30\n",
      "Q Squared: -0.89 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.733 (22/30) \n",
      "Positive: 0.263 (5/19) \n",
      "Num pos after sampling: 6\n",
      "Num neg after sampling: 14\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.857 (12/14) \n",
      "Positive: 0.667 (4/6) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='AM', n_top_genes=100, jumble_train=False, method='RandomForests', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  AM\n",
      "Method =  RandomForests\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 19\n",
      "Num neg after sampling: 30\n",
      "---- Entire training set is jumbled ----\n",
      "\n",
      "Q Squared: -1.32 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.700 (21/30) \n",
      "Positive: 0.053 (1/19) \n",
      "Num pos after sampling: 6\n",
      "Num neg after sampling: 14\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.857 (12/14) \n",
      "Positive: 0.000 (0/6) \n"
     ]
    }
   ],
   "source": [
    "genesK = run_model(celltype='AM', n_top_genes=100, jumble_train=True, method='RandomForests', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='LOO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.093784944486270089, 0.35335256719661567)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(genesK[0], genesK[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10f4208d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqUAAAHcCAYAAADvHdOmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bc99110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(genesK[0], genesK[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  CD163\n",
      "Method =  PLSR\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 15\n",
      "Num neg after sampling: 31\n",
      "---- Entire training set is jumbled ----\n",
      "\n",
      "Q Squared: -0.64 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.730 (27/37) \n",
      "Positive: 0.154 (2/13) \n",
      "Num pos after sampling: 7\n",
      "Num neg after sampling: 5\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.800 (4/5) \n",
      "Positive: 0.000 (0/7) \n"
     ]
    }
   ],
   "source": [
    "genesK = get_accuracy_stats(celltype='CD163', n_top_genes=10, jumble_train=True, #method='RandomForests', \n",
    "                       norm='zscore', random_genes=False, jumble_test=False, sampling='', cross_validation='Kfold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  CD163\n",
      "Method =  PLSR\n",
      "Normalization =  False\n",
      "Num pos after sampling: 15\n",
      "Num neg after sampling: 31\n",
      "---- Entire training set is jumbled ----\n",
      "\n",
      "Q Squared: -0.71 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.622 (23/37) \n",
      "Positive: 0.231 (3/13) \n",
      "Num pos after sampling: 7\n",
      "Num neg after sampling: 5\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 1.000 (5/5) \n",
      "Positive: 0.000 (0/7) \n"
     ]
    }
   ],
   "source": [
    "genesK = get_accuracy_stats(celltype='CD163', n_top_genes=10, jumble_train=True, #method='RandomForests', \n",
    "                       norm=False, random_genes=False, jumble_test=False, sampling='', cross_validation='Kfold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  CD163\n",
      "Method =  PLSR\n",
      "Normalization =  False\n",
      "Num pos after sampling: 15\n",
      "Num neg after sampling: 31\n",
      "Q Squared: 0.12 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.800 (28/35) \n",
      "Positive: 0.533 (8/15) \n",
      "\n",
      " **Genes for testing are RANDOM** \n",
      "\n",
      "Num pos after sampling: 7\n",
      "Num neg after sampling: 5\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.800 (4/5) \n",
      "Positive: 0.286 (2/7) \n"
     ]
    }
   ],
   "source": [
    "genesK = get_accuracy_stats(celltype='CD163', n_top_genes=10, jumble_train=False, #method='RandomForests', \n",
    "                       norm=False, random_genes=True, jumble_test=False, sampling='', cross_validation='Kfold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =  CD163\n",
      "Method =  PLSR\n",
      "Normalization =  zscore\n",
      "Num pos after sampling: 15\n",
      "Num neg after sampling: 31\n",
      "Q Squared: -0.27 \n",
      "\n",
      "\n",
      "===== Training ======\n",
      "Negative: 0.821 (32/39) \n",
      "Positive: 0.364 (4/11) \n",
      "\n",
      " **Genes for testing are RANDOM** \n",
      "\n",
      "Num pos after sampling: 7\n",
      "Num neg after sampling: 5\n",
      "\n",
      "===== Testing =====\n",
      "Negative: 0.800 (4/5) \n",
      "Positive: 0.000 (0/7) \n"
     ]
    }
   ],
   "source": [
    "genesK = get_accuracy_stats(celltype='CD163', n_top_genes=5, jumble_train=False, #method='RandomForests', \n",
    "                       norm='zscore', random_genes=True, jumble_test=False, sampling='', cross_validation='Kfold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different normalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cluster(df_train_top, dfm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cluster(df, dfm):\n",
    "    \n",
    "    dfz = df.apply(zscore, axis=1)\n",
    "    dfz = dfz.apply(zscore, axis=0)\n",
    "    dfplot = dfz.T\n",
    "\n",
    "    dfplot = dfplot.rename(columns={c: dfm.loc[c,'Pneum'] for c in dfplot.columns})\n",
    "\n",
    "    dfplot.columns\n",
    "    fig = sns.clustermap(dfplot, col_cluster=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cluster(df_test_top, dfm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_cluster(df_train_top, dfm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#determine the best number of genes for the training model\n",
    "def plotQ2(r=np.arange(10,205,5), celltype=False, z=True):\n",
    "    Q2s = []\n",
    "    for n_top_genes in tqdm(r):\n",
    "        X, y, genes, df, dfm = geteven_xy(train_fpkm, meta, celltype=celltype, norm='zscore')\n",
    "        neg_err, pos_err, Q2, neg_corr, pos_corr, vip_inds = calc_metrics(X, y, \n",
    "                                                        n_pcs=5, n_top_genes=n_top_genes)\n",
    "        Q2s.append(Q2)\n",
    "    plt.plot(r, Q2s)\n",
    "    return None #Q2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotQ2(r=np.arange(10,155,10), celltype='AM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotQ2(r=np.arange(10,155,5), celltype='CD163')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vipp(x, y, t, w):\n",
    "\n",
    "    \"\"\"\n",
    "    From original MATLAB code\n",
    "    See https://code.google.com/p/carspls/\n",
    "\n",
    "    #+++ vip=vipp(x,y,t,w);\n",
    "    #+++ t: scores, which can be obtained by pls_nipals.m\n",
    "    #+++ w: weight, which can be obtained by pls_nipals.m\n",
    "    #+++ to calculate the vip for each variable to the response;\n",
    "    #+++ vip=sqrt(p*q/s);\n",
    "    \"\"\"\n",
    "    #initializing\n",
    "    [p, h] = w.shape\n",
    "    co = np.matrix(np.zeros([1, h]))\n",
    "\n",
    "    # Calculate s\n",
    "    for ii in range(h):\n",
    "        corr = np.corrcoef(y, t[:, ii], rowvar=0)\n",
    "        co[0, ii] = corr[0, 1]**2\n",
    "    s = np.sum(co)\n",
    "\n",
    "    # Calculate q\n",
    "    # This has been linearized to replace the original nested for loop\n",
    "    w_power = np.power(w, 2)\n",
    "    d = np.multiply(w_power, co)\n",
    "    q = np.sum(d, 1)\n",
    "    vip = np.sqrt(p*q/s)\n",
    "    return vip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_Q2_kftest(X, y, k=5, n_rand=10, n_pcs=5, method='PLSR'):\n",
    "    n_samples = len(X)\n",
    "    pred = []\n",
    "    Q2s = []\n",
    "    for ki in range(n_rand):\n",
    "        sample_ind = range(n_samples)\n",
    "        withheld_samples = random.sample(sample_ind, k)\n",
    "        lo_samples = X[withheld_samples]\n",
    "        for s in withheld_samples:\n",
    "            sample_ind.remove(s)\n",
    "        X_t = X[(sample_ind)]\n",
    "        y_t = y[(sample_ind)]\n",
    "        if method=='PLSR':\n",
    "            model = PLSRegression(n_pcs, scale=False)\n",
    "        elif method=='RF':\n",
    "            model = RandomForestClassifier()\n",
    "        elif method=='SVM':\n",
    "            model = SVC()\n",
    "        else:\n",
    "            print \"Method not found\"\n",
    "        model.fit(X_t, y_t)\n",
    "\n",
    "        for s in withheld_samples:\n",
    "            pred.append(float(model.predict(X[s])))\n",
    "        #Eprint \"samples\", withheld_samples\n",
    "        #Eprint \"pred\", pred\n",
    "        num = sum([(pred[i] - y[s])**2 for i, s in enumerate(withheld_samples)])\n",
    "        den = sum([(y[s] - np.mean(y[withheld_samples]))**2 for s in withheld_samples])\n",
    "        Q2 = float(1- num/den)\n",
    "        Q2s.append(Q2)\n",
    "    print \"Q^2: mean={0: .2f}, std={1: .2f}\".format(np.mean(Q2s),np.std(Q2s))\n",
    "    #return [float(abs(pred[p] - y[p])) for p in range(len(pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
